# -*- coding: utf-8 -*-
"""Double Conv layer

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sW_kZvIljd0t9UbX7bQOn6c3eyg6q9Up
"""

import torch
from torch import nn
from partial_conv import PartialConv

class DoublePConv(nn.Module):
  '''
  --------------------DESCRIPTION---------------------
  Return Double Partial Convolutional layers given the input parameters

  --------------------PARAMETERS----------------------
  in_channels: input channels for the first convolutional layer
  out_channels: output channels for the second convolutional layer
  kernel_size: kernel size to use for both the layers
  activation: activaiton to apply to both the layers, should pass a activation function and not string.
  padding: padding to be applied to the inputs, by default no padding.
  batch_norm: if True applies nn.BatchNorm2d() after every Convolutional layer.
  '''
  def __init__(self, in_channels, out_channels, kernel_size, activation, padding='same', batch_norm=True):
    super().__init__()

    # first conv
    self.pconv1 = PartialConv(in_channels, out_channels, kernel_size, padding=padding)
    self.bn1 = nn.BatchNorm2d(out_channels)
    self.act1 = activation()

    # second conv
    self.pconv2 = PartialConv(out_channels, out_channels, kernel_size, padding=padding)
    self.bn2 = nn.BatchNorm2d(out_channels)
    self.act2 = activation()


  def forward(self, input, mask):

    x, m = self.pconv1(input, mask)
    x = self.bn1(x)
    x = self.act1(x)

    x, m = self.pconv2(x, m)
    x = self.bn2(x)
    out = self.act2(x)

    return x, m

class DoubleConv(nn.Module):
  '''
  --------------------DESCRIPTION---------------------
  Return Double Convolutional layers given the input parameters

  --------------------PARAMETERS----------------------
  in_channels: input channels for the first convolutional layer
  out_channels: output channels for the second convolutional layer
  kernel_size: kernel size to use for both the layers
  activation: activaiton to apply to both the layers, should pass a activation function and not string.
  padding: padding to be applied to the inputs, by default no padding.
  batch_norm: if True applies nn.BatchNorm2d() after every Convolutional layer.
  '''
  def __init__(self, in_channels, out_channels, kernel_size, activation, padding='same', batch_norm=True):
    super().__init__()

    # first conv
    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)
    self.bn1 = nn.BatchNorm2d(out_channels)
    self.act1 = activation()

    # second conv
    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding)
    self.bn2 = nn.BatchNorm2d(out_channels)
    self.act2 = activation() 


  def forward(self, input):

    x = self.conv1(input)
    x = self.bn1(x)
    x = self.act1(x)

    x = self.conv2(x)
    x = self.bn2(x)
    out = self.act2(x)

    return x